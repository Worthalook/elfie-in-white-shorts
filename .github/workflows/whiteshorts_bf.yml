name: WS once-off backfill

on:
 
  workflow_dispatch:
    inputs:
      start:
        description: "Backfill start date (YYYY-MM-DD or DD/MM/YYYY)"
        required: true
        default: "01/10/2025"
      end:
        description: "Backfill end date (YYYY-MM-DD or DD/MM/YYYY)"
        required: true
        default: "29/10/2025"
      build_dashboards:
        description: "Build dashboards after backfill?"
        required: true
        default: "true"
        type: choice
        options: ["true", "false"]

concurrency:
  group: ws-backfill
  cancel-in-progress: false

jobs:
  backfill:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      actions: read
      checks: read
    env:
      PIP_DISABLE_PIP_VERSION_CHECK: "1"
      PYTHONUNBUFFERED: "1"
      WS_SLATES_DIR: data/slates
      WS_PARQUET_DIR: data/parquet
      WS_CURRENT_SEASON_PARQUET: data/current_season.parquet
      DUCKDB_PATH: data/white_shorts.duckdb
      SPORTS_DATA_BASE: https://api.sportsdata.io
      SPORTS_DATA_API_KEY: ${{ secrets.SPORTS_DATA_API_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      - name: Add src to PYTHONPATH
        run: echo "PYTHONPATH=$GITHUB_WORKSPACE/src:$PYTHONPATH" >> $GITHUB_ENV

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install typer duckdb joblib scikit-learn statsmodels pandas pyarrow requests

      - name: Normalize input dates & build list
        id: dates
        env:
          INPUT_START: ${{ github.event.inputs.start }}
          INPUT_END: ${{ github.event.inputs.end }}
        run: |
            python - <<'PY'
            import os
            import sys
            import pandas as pd
            s_raw = (os.getenv("INPUT_START") or "").strip()
            e_raw = (os.getenv("INPUT_END") or "").strip()
            s = pd.to_datetime(s_raw, dayfirst=True, errors="coerce")
            e = pd.to_datetime(e_raw, dayfirst=True, errors="coerce")
            if pd.isna(s) or pd.isna(e) or s > e:
                raise SystemExit(f"Invalid range: start='{s_raw}' end='{e_raw}'")
            rng = pd.date_range(s.normalize(), e.normalize(), freq="D")
            with:
            open(os.environ["GITHUB_OUTPUT"], "a") as fh:
                fh.write(f"start_norm={rng.min().date()}\n")
                fh.write(f"end_norm={rng.max().date()}\n")
                fh.write(f"days={','.join(d.date().isoformat() for d in rng)}\n") 
            PY

      - name: Upload API probe artifacts
        uses: actions/upload-artifact@v4
        with:
           name: api-probe-actuals
           path: data/debug/*.*
           retention-days: 7

      - name: Backfill actuals into DuckDB (day-by-day)
        env:    
            DAYS: ${{ steps.dates.outputs.days }}  # produced by your earlier "Normalize input dates" step
        run: |
            IFS=',' read -ra LIST <<< "$DAYS"
            echo "Backfilling ${#LIST[@]} day(s): ${DAYS}"
            FAIL=0
            for D in "${LIST[@]}"; do
                echo "==> Logging actuals for ${D}"
                # skip if it's today (actuals incomplete)
                if [ "$(date -u -d "$D" +%F)" = "$(date -u +%F)" ]; then
                echo "   skipping today (${D}); actuals not finalized yet."
                continue
                fi
                if ! python -m white_shorts.cli.update_history "${D}"; then
                echo "::warning title=update_history failed::Date ${D} failed, continuing"
                FAIL=$((FAIL+1))
                fi
                sleep 0.3
            done
            echo "Backfill done, failed days: ${FAIL}"


      - name: Build dashboards (60d)
        run: |
            python -m white_shorts.cli.dashboards build --days 60 --out data/dashboards


      - name: Verify actuals written & overlap with predictions
        run: |
            python - <<'PY'
            import os, duckdb
            db = os.getenv("DUCKDB_PATH","data/white_shorts.duckdb")
            con = duckdb.connect(db)

            def cnt(tbl):
                try: return con.execute(f"SELECT COUNT(*) FROM {tbl}").fetchone()[0]
                except: return "missing"

            print("counts:", {"fact_predictions": cnt("fact_predictions"), "fact_actuals": cnt("fact_actuals")})

            # show last few dates for each
            try:
                print("recent actual dates:")
                print(con.execute("SELECT DISTINCT date FROM fact_actuals ORDER BY date DESC LIMIT 10").fetchdf().to_string(index=False))
            except Exception as e:
                print("fact_actuals missing:", e)

            try:
                print("recent pred dates:")
                print(con.execute("SELECT DISTINCT date FROM fact_predictions ORDER BY date DESC LIMIT 10").fetchdf().to_string(index=False))
            except Exception as e:
                print("fact_predictions missing:", e)

            # normalized join
            q = """
            WITH p AS (
                SELECT CAST(date AS DATE) AS d, CAST(game_id AS BIGINT) AS gid,
                        UPPER(TRIM(team)) AS t, UPPER(TRIM(opponent)) AS o,
                        CAST(player_id AS BIGINT) AS pid, target
                FROM fact_predictions
                WHERE date >= (CURRENT_DATE - INTERVAL 30 DAY)
            ),
            a AS (
                SELECT CAST(date AS DATE) AS d, CAST(game_id AS BIGINT) AS gid,
                        UPPER(TRIM(team)) AS t, UPPER(TRIM(opponent)) AS o,
                        CAST(player_id AS BIGINT) AS pid, target, actual
                FROM fact_actuals
                WHERE date >= (CURRENT_DATE - INTERVAL 30 DAY)
            )
            SELECT p.d AS date,
                    COUNT(*) AS preds,
                    SUM(CASE WHEN a.actual IS NOT NULL THEN 1 ELSE 0 END) AS matched
            FROM p LEFT JOIN a USING (d,gid,t,o,pid,target)
            GROUP BY 1 ORDER BY 1 DESC LIMIT 14
            """
            try:
                print("join sample (last 14 days):")
                print(con.execute(q).fetchdf().to_string(index=False))
            except Exception as e:
                print("Join diagnostic failed:", e)
            con.close()
            PY




      - name: Diagnose DuckDB path and row counts
        run: |
            echo "DuckDB path (env): $DUCKDB_PATH"
            if [ ! -f "$DUCKDB_PATH" ]; then
              echo "::error file=$DUCKDB_PATH::DuckDB file not found at that path."; exit 1
            fi
            ls -lh "$DUCKDB_PATH" || true

            python - <<'PY'
            import os, sys, pandas as pd, duckdb
            from white_shorts.config import settings
            db_env = os.getenv("DUCKDB_PATH")
            print("settings.DUCKDB_PATH =", settings.DUCKDB_PATH)
            print("env DUCKDB_PATH      =", db_env)

            con = duckdb.connect(db_env or settings.DUCKDB_PATH)
            try:
                # Show attached DBs
                print(con.execute("PRAGMA database_list").fetchdf().to_string(index=False))

                # Table counts (handle missing tables gracefully)
                def count_safe(tbl):
                    try:
                        return con.execute(f"SELECT COUNT(*) AS n FROM {tbl}").fetchone()[0]
                    except Exception as e:
                        return f"missing ({e.__class__.__name__})"

                fa = count_safe("fact_actuals")
                fp = count_safe("fact_predictions")
                print("fact_actuals rows: ", fa)
                print("fact_predictions rows:", fp)
            finally:
                con.close()
            PY


      - name: Ensure .gitignore allows DuckDB and data (but still ignore models/)
        run: |
            grep -q '^models/$' .gitignore || echo 'models/' >> .gitignore
            if grep -qE '^(data/|/data/)$' .gitignore; then
              echo "Removing 'data/' from .gitignore so DuckDB can be tracked."
              sed -i.bak '/^data\/$/d' .gitignore || true
              sed -i.bak '/^\/data\/$/d' .gitignore || true
              rm -f .gitignore.bak
            fi
            git status --porcelain


      # If the DB didn't change (idempotent backfill), ensure a tiny update so the file hashes differ
      - name: Bump repo_state in DuckDB to ensure commit (backfill only)
        run: |
          python - <<'PY'
          import os, duckdb, pandas as pd
          db = os.getenv("DUCKDB_PATH", "data/white_shorts.duckdb")
          con = duckdb.connect(db)
          try:
              con.execute("CREATE TABLE IF NOT EXISTS repo_state (ts TIMESTAMP, note VARCHAR)")
              con.execute("INSERT INTO repo_state VALUES (now(), ?)", [f"backfill run on {os.getenv('GITHUB_RUN_ID','local')}"])
              # Force a checkpoint to flush WAL (ensures file bytes change)
              con.execute("CHECKPOINT")
              n = con.execute("SELECT COUNT(*) FROM repo_state").fetchone()[0]
              print("repo_state rows:", n)
          finally:
            con.close()
          PY

      - name: Show DuckDB size after bump
        run: |
         ls -lh "$DUCKDB_PATH" || true

      - name: Build dashboards (optional)
        if: ${{ github.event.inputs.build_dashboards == 'true' }}
        run: |
          python -m white_shorts.cli.dashboards build --days 60 --out data/dashboards

      - name: Upload dashboards artifacts
        if: ${{ github.event.inputs.build_dashboards == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: dashboards-backfill-${{ steps.dates.outputs.start_norm }}_to_${{ steps.dates.outputs.end_norm }}
          path: |
            data/dashboards/*.csv
            data/dashboards/*.html
          retention-days: 14

      - name: Ensure models are ignored
        run: |
          grep -q '^models/$' .gitignore || echo 'models/' >> .gitignore

      - name: Commit dashboards (if built)
        if: ${{ github.event.inputs.build_dashboards == 'true' }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -A data/dashboards/*.csv || true
          git add -A data/dashboards/*.html || true
          if git diff --cached --quiet; then
            echo "No dashboard changes to commit."
          else
            git commit -m "Backfill dashboards for ${{ steps.dates.outputs.start_norm }} → ${{ steps.dates.outputs.end_norm }} [skip ci]"
            git push origin HEAD:${{ github.ref_name }}
          fi

      - name: Commit DuckDB state
        run: |
            git config user.name "github-actions[bot]"
            git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
            if [ -f "${{ env.DUCKDB_PATH }}" ]; then
              git add -f "${{ env.DUCKDB_PATH }}"
              git status --porcelain "${{ env.DUCKDB_PATH }}"
              if git diff --cached --quiet -- "${{ env.DUCKDB_PATH }}"; then
                echo "No DuckDB changes to commit."
              else
                git commit -m "Backfill: update DuckDB state [${{ steps.dates.outputs.start_norm }}→${{ steps.dates.outputs.end_norm }}] [skip ci]"
                git push origin HEAD:${{ github.ref_name }}
              fi
            else
              echo "DuckDB not found at ${{ env.DUCKDB_PATH }}; skipping commit."
            fi


