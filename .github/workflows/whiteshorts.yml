name: WS daily pipeline (AEST)

on:
  push:
    branches: [ "initial-testing" ]    
  workflow_dispatch:
    inputs:
      run_training:
        description: "Also (re)train QRF models?"
        required: true
        default: "false"
        type: choice
        options: ["false", "true"]
      override_date:
        description: "Slate date (YYYY-MM-DD or DD/MM/YYYY). Leave empty for 'tomorrow' in Australia/Melbourne."
        required: false
        default: ""
  #schedule:
    # 09:05 Melbourne time daily ~= 22:05 UTC previous day (adjust as you prefer)
   # - cron: "05 22 * * *"

jobs:
  ws-pipeline:
    runs-on: ubuntu-latest
    permissions:
      contents: write    # needed to push the CSV back to the repo
      actions: read
      checks: read
    env:
      PIP_DISABLE_PIP_VERSION_CHECK: "1"
      PYTHONUNBUFFERED: "1"
      WS_SLATES_DIR: data/slates
      WS_PARQUET_DIR: data/parquet
      WS_CURRENT_SEASON_PARQUET: data/current_season.parquet
      # Never commit models; ensure models/ is in .gitignore
      SPORTS_DATA_BASE: https://api.sportsdata.io
      # API key should be stored in repo/org secrets
      SPORTS_DATA_API_KEY: ${{ secrets.SPORTS_DATA_API_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          # Keep credentials so we can push with GITHUB_TOKEN
          persist-credentials: true
              
      - name: Add src to PYTHONPATH
        run: echo "PYTHONPATH=${PYTHONPATH}:$GITHUB_WORKSPACE/src" >> $GITHUB_ENV

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          # If your project doesnâ€™t have a pinned requirements, install the libs you use:
          pip install typer duckdb joblib scikit-learn statsmodels pandas pyarrow requests

      - name: Compute Melbourne dates
        id: dates
        run: |
          python - << 'PY'
          from datetime import datetime, timedelta
          from zoneinfo import ZoneInfo
          import os
          override = os.getenv("INPUT_OVERRIDE_DATE","").strip()
          tz = ZoneInfo("Australia/Melbourne")
          now_au = datetime.now(tz).replace(hour=9, minute=0, second=0, microsecond=0)
          if override:
              # accept both YYYY-MM-DD and DD/MM/YYYY
              import pandas as pd
              d = pd.to_datetime(override, dayfirst=True, errors="coerce")
              if pd.isna(d): raise SystemExit(f"Unparseable override_date: {override}")
              slate = d.date()
          else:
              slate = (now_au - timedelta(days=1)).date()  # yesterday AU
          yday  = (now_au - timedelta(days=2)).date()      # -2 AU
          print(f"slate={slate}  yesterday={yday}")
          with open(os.environ["GITHUB_OUTPUT"], "a") as fh:
              fh.write(f"slate={slate}\n")
              fh.write(f"yesterday={yday}\n")
          PY
        env:
          INPUT_OVERRIDE_DATE: ${{ github.event.inputs.override_date }}

      # ---------- PREP DATA ----------
      - name: Fetch slate (players & games) for slate date
        run: |
          python -m white_shorts.cli.fetch_slate "${{ steps.dates.outputs.slate }}"

      - name: Update current-season with yesterday actuals
        run: |
          python -m white_shorts.cli.update_history "${{ steps.dates.outputs.yesterday }}"

      # ---------- (OPTIONAL) TRAIN ----------
      - name: Train QRF models (optional)
        if: ${{ github.event_name == 'workflow_dispatch' && inputs.run_training == 'true' }}
        run: |
          python -m white_shorts.cli.train_qrf all --version 0.3.0

      # ---------- PREDICT ----------
      - name: Predict from saved slate
        run: |
          SLATE_PATH="data/slates/slate_${{ steps.dates.outputs.slate }}.parquet"
          if [ ! -f "$SLATE_PATH" ]; then
            echo "Slate not found: $SLATE_PATH"; exit 1
          fi
          python -m white_shorts.cli.predict_from_slate slate "$SLATE_PATH" \
            --ytd-csv "data/NHL_YTD.csv" \
            --out-dir "${{ env.WS_PARQUET_DIR }}"

      # ---------- COLLECT & UPLOAD ARTIFACT ----------
      - name: List prediction CSVs
        run: ls -lh "${{ env.WS_PARQUET_DIR }}" || true

      - name: Upload prediction CSV(s) as workflow artifact
        uses: actions/upload-artifact@v4
        with:
          name: predictions-${{ steps.dates.outputs.slate }}
          path: |
            ${{ env.WS_PARQUET_DIR }}/*.csv
          if-no-files-found: error
          retention-days: 14

      # ---------- COMMIT CSVs TO REPO (NOT MODELS) ----------
      - name: Ensure models are ignored
        run: |
          grep -q '^models/$' .gitignore || echo 'models/' >> .gitignore

      - name: Commit prediction CSV(s) to branch
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          # Only add CSVs under data/parquet
          git add -A data/parquet/*.csv || true
          # Skip commit if nothing changed
          if git diff --cached --quiet; then
            echo "No new CSVs to commit."
          else
            git commit -m "Add predictions for ${{ steps.dates.outputs.slate }} [skip ci]"
            git push origin HEAD:${{ github.ref_name }}
          fi
