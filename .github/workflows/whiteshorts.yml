name: WS daily pipeline (AET)

on:
  #push:
    #branches: [ "main" ]

  workflow_dispatch:
    inputs:
      run_training:
        description: "Also (re)train QRF models?"
        required: true
        default: "true"
        type: choice
        options: ["true", "false"]
      override_date:
        description: "Slate date (YYYY-MM-DD or DD/MM/YYYY). Leave empty for 'tomorrow' in Australia/Melbourne."
        required: false
        default: ""
  schedule:
     # 09:05 Melbourne time daily ~= 22:05 UTC previous day
     - cron: "19 22 * * *"

jobs:
  ws-pipeline:
    runs-on: ubuntu-latest
    permissions:
      contents: write    # needed to push data back to the repo
      actions: read
      checks: read
    env:
      PIP_DISABLE_PIP_VERSION_CHECK: "1"
      PYTHONUNBUFFERED: "1"
      WS_SLATES_DIR: data/slates
      WS_PARQUET_DIR: data/parquet
      WS_CURRENT_SEASON_PARQUET: data/current_season.parquet
      DUCKDB_PATH: data/white_shorts.duckdb     # <— ensure this matches settings.DUCKDB_PATH
      SPORTS_DATA_BASE: https://api.sportsdata.io
      SPORTS_DATA_API_KEY: ${{ secrets.SPORTS_DATA_API_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true
          # If you later move DuckDB to Git LFS, also add:
          # lfs: true

      - name: Add src to PYTHONPATH
        run: echo "PYTHONPATH=$GITHUB_WORKSPACE/src:$PYTHONPATH" >> $GITHUB_ENV

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Setup Java for Flutter
        uses: actions/setup-java@v4
        with:
            distribution: 'temurin'
            java-version: '17'
      
      - name: Build App apk
        working-directory: ./packages/whiteshorts_app2
        run: |
            flutter build apk
    
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install typer duckdb joblib scikit-learn statsmodels pandas pyarrow requests
      
      - name: Build wheel for whiteshorts_broadcast
        working-directory: ./packages/whiteshorts_broadcast   # ⬅️ adjust
        run: |
          python -m pip install --upgrade pip build
          python -m build --wheel
          ls -l dist

      - name: Upload wheel artifact
        uses: actions/upload-artifact@v4
        with:
          name: whiteshorts_broadcast-wheel
          path: packages/whiteshorts_broadcast/dist/*.whl

      - name: Download wheel artifact
        uses: actions/download-artifact@v4
        with:
          name: whiteshorts_broadcast-wheel
          path: ./wheels

      - name: Install whiteshorts_broadcast wheel
        run: |
          python -m pip install --upgrade pip
          pip install ./wheels/*.whl
          python - <<'PY'
          import whiteshorts_broadcast as wsb
          print("whiteshorts_broadcast OK ->", wsb.__file__)
          PY
  
      - name: Compute Melbourne dates
        id: dates
        env:
          INPUT_OVERRIDE_DATE: ${{ github.event.inputs.override_date }}
        run: |
          python - << 'PY'
          from datetime import datetime, timedelta
          from zoneinfo import ZoneInfo
          import os
          try:
              import pandas as pd
          except Exception:
              raise SystemExit("pandas required in this step")

          override = (os.getenv("INPUT_OVERRIDE_DATE") or "").strip()
          tz = ZoneInfo("Australia/Melbourne")
          now_au = datetime.now(tz).replace(hour=9, minute=0, second=0, microsecond=0)

          if override:
              d = pd.to_datetime(override, dayfirst=True, errors="coerce")
              if pd.isna(d):
                  raise SystemExit(f"Unparseable override_date: {override}")
              slate = d.date()
          else:
              slate = (now_au).date()  - timedelta(days=1)  # 'tomorrow' AU for projections'
          
          # yesterday relative to slate
          yday = (slate - timedelta(days=1))               # yesterday relative to slate
          
          with open(os.environ["GITHUB_OUTPUT"], "a") as fh:
              fh.write(f"slate={slate}\n")
              fh.write(f"yesterday={yday}\n")

          PY

      # ---------- PREP DATA ----------
      - name: Fetch slate (players & games) for slate date
        run: |
          python -m white_shorts.cli.fetch_slate "${{ steps.dates.outputs.slate }}"

      # ---------- UPDATE ACTUALS FIRST (so dashboards have data) ----------
      - name: Update current-season with yesterday actuals
        run: |
          python -m white_shorts.cli.update_history "${{ steps.dates.outputs.yesterday }}"
      # ---------- COMMIT SLATE ARTIFACT TO BRANCH ----------
      - name: Commit slates to branch
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -A data/slates/*.parquet || true
        
          if git diff --cached --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Slate artifacts for ${{ steps.dates.outputs.slate }} [skip ci]"
            git push origin HEAD:${{ github.ref_name }}
          fi
      # ---------- (OPTIONAL) TRAIN ----------
      - name: Train QRF models (optional)
        #if: ${{ github.event_name == 'workflow_dispatch' && inputs.run_training == 'true' }}
        run: |
          python -m white_shorts.cli.train_qrf all --version 0.3.0
          # Train all four targets, prefer DuckDB (120d), fall back to API (30d)
          #python -m white_shorts.cli.train_qrf all --version 0.3.1  --api-backfill-days 20


      # ---------- PREDICT ----------
      - name: Predict from saved slate
        run: |
          SLATE_PATH="data/slates/slate_${{ steps.dates.outputs.slate }}.parquet"
          #echo "slate_${{ steps.dates.outputs.slate }}.parquet"
          # SLATE_PATH_2="data/slates/slate_2025-04-11.parquet"
          
          # SLATE_PATH="data/slates/slate_2025-01-.parquet"
          if [ ! -f "$SLATE_PATH" ]; then  
            echo "Slate not found: $SLATE_PATH"
            #$SLATE_PATH = SLATE_PATH_2  
          fi

          #if [ ! -f "$SLATE_PATH_2" ]; then  
            #echo "Slate not found: $SLATE_PATH_2";exit 1
          #fi

          python -m white_shorts.cli.predict_from_slate "$SLATE_PATH" --ytd-csv "data/NHL_2023_24.csv"  --out-dir "${{ env.WS_PARQUET_DIR }}"

      #----------write to supabase (for mobile broadcast)
 
      - name: Publish predictions to Supabase
        id: Publish_Results_Predictions
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: |
              python - << 'PY'
              import os, pandas as pd
              from whiteshorts_broadcast import publish_results, BroadcastConfig, find_latest_csv
              #csv_path = find_latest_csv("data/parquet/predictions_*.csv") 
              df = pd.read_csv('data/parquet/predictions_2025-11-11_462337372233715334.csv') #csv_path
              cfg = BroadcastConfig(
              backend='supabase',
              supabase_url=os.environ['SUPABASE_URL'],
              supabase_anon_key=os.environ['SUPABASE_SERVICE_KEY'],
              supabase_table='predictions',
              upsert_on=['date','player_id','target'])
              publish_results(df, cfg)
              PY
      # ---------- COLLECT & UPLOAD PRED ARTIFACT ----------
      - name: List prediction CSVs
        run: ls -lh "${{ env.WS_PARQUET_DIR }}" || true

      - name: Upload prediction CSV(s) as workflow artifact
        uses: actions/upload-artifact@v4
        with:
          name: predictions-${{ steps.dates.outputs.slate }}
          path: |
            ${{ env.WS_PARQUET_DIR }}/*.csv
          if-no-files-found: error
          retention-days: 14

      # ---------- GIT HYGIENE ----------
      - name: Ensure models are ignored
        run: |
          grep -q '^models/$' .gitignore || echo 'models/' >> .gitignore

      # ---------- DASHBOARDS (build once, with actuals now present) ----------
      - name: Build dashboards (14d)
        run: |
          python -m white_shorts.cli.dashboards build --days 14 --out data/dashboards

      - name: Upload dashboards (csv + html)
        uses: actions/upload-artifact@v4
        with:
          name: dashboards-${{ steps.dates.outputs.slate }}
          path: |
            data/dashboards/*.csv
            data/dashboards/*.html
          retention-days: 14

      # ---------- COMMIT DATA ARTIFACTS TO BRANCH ----------
      - name: Commit prediction CSV(s) & dashboards to branch
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add -A data/parquet/*.csv || true
          git add -A data/dashboards/*.csv || true
          git add -A data/dashboards/*.html || true
          if git diff --cached --quiet; then
            echo "No CSV/HTML changes to commit."
          else
            git commit -m "Data artifacts for ${{ steps.dates.outputs.slate }} [skip ci]"
            git push origin HEAD:${{ github.ref_name }}
          fi

      # ---------- ✅ COMMIT DUCKDB (so joins work next run) ----------
      - name: Commit DuckDB state
        run: |
          # Consider Git LFS for binary DB if it grows large: git lfs track "data/*.duckdb"
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          if [ -f "${{ env.DUCKDB_PATH }}" ]; then
            git add -f "${{ env.DUCKDB_PATH }}"
            if git diff --cached --quiet; then
              echo "No DuckDB changes to commit."
            else
              git commit -m "Update DuckDB state for ${{ steps.dates.outputs.slate }} [skip ci]"
              git push origin HEAD:${{ github.ref_name }}
            fi
          else
            echo "DuckDB not found at ${{ env.DUCKDB_PATH }}; skipping commit."
          fi
